{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWS40vyx70EEMHNy2KKmH7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkpsRccq3QoP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get data and convert to UTF-8"
      ],
      "metadata": {
        "id": "X7eUXv_M9mF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
        "dataset = tf.keras.utils.get_file(\"shakespeare.txt\", url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji9mOQ_p8Cev",
        "outputId": "d7658ed6-4f14-4387-dc8d-ef81955c13d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(dataset, \"rb\").read().decode(\"utf-8\")\n",
        "\n",
        "print(f\"Num of characters:{len(text)}\")\n",
        "print(\"-\" * 50)\n",
        "print(text[:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbjmV45g8egM",
        "outputId": "d09c126a-1f12-46ed-e60c-7961845d80f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of characters:1115394\n",
            "--------------------------------------------------\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a dictionary mapping chars to indices"
      ],
      "metadata": {
        "id": "4Jb_boRo9r-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text.lower())) # returns a list with unique characters sorted alphabetically\n",
        "ids_to_char = np.array(vocab) # convert to numpy array\n",
        "char_to_ids = {char:i for i, char in enumerate(ids_to_char)} #dictionary of chars to indexes"
      ],
      "metadata": {
        "id": "Qbfpd1N39ME1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert each character in text to its interger value in the vocabulary"
      ],
      "metadata": {
        "id": "NNIv_CPW_KOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_ids = np.array([char_to_ids[char] for char in text.lower()])\n",
        "print(text_ids[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlJaECTU-Yge",
        "outputId": "846dc5c8-364c-41f2-cd9d-73241b03d8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18 21 30 31 32  1 15 21 32 21 38 17 26 10  0 14 17 18 27 30 17  1 35 17\n",
            "  1 28 30 27 15 17 17 16  1 13 26 37  1 18 33 30 32 20 17 30  6  1 20 17\n",
            " 13 30  1 25 17  1 31 28 17 13 23  8  0  0 13 24 24 10  0 31 28 17 13 23\n",
            "  6  1 31 28 17 13 23  8  0  0 18 21 30 31 32  1 15 21 32 21 38 17 26 10\n",
            "  0 37 27 33  1 13 30 17  1 13 24 24  1 30 17 31 27 24 34 17 16  1 30 13\n",
            " 32 20 17 30  1 32 27  1 16 21 17  1 32 20 13 26  1 32 27  1 18 13 25 21\n",
            " 31 20 12  0  0 13 24 24 10  0 30 17 31 27 24 34 17 16  8  1 30 17 31 27\n",
            " 24 34 17 16  8  0  0 18 21 30 31 32  1 15 21 32 21 38 17 26 10  0 18 21\n",
            " 30 31 32  6  1 37 27 33]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split data into chunks - creating sequences of certain length\n",
        "\n",
        "The sequence is the input. Label is one place shifted right."
      ],
      "metadata": {
        "id": "F0ocyhUwAbQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(chunk):\n",
        "  input_chunk = chunk[:-1] #everything but last character in this chunk\n",
        "  label_chunk = chunk[1:] #label to char index 0 in chunk\n",
        "  return input_chunk, label_chunk"
      ],
      "metadata": {
        "id": "EOOrCJl0AFxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LEN = 64 # add 1 to this length because chunk needs to bring 64 characters\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "samples = tf.data.Dataset.from_tensor_slices(text_ids).batch(SEQUENCE_LEN + 1, drop_remainder=True).map(split_data).shuffle(BUFFER_SIZE)"
      ],
      "metadata": {
        "id": "6qEaFdPaCno1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into training and test sets"
      ],
      "metadata": {
        "id": "yuFhrI3TE7xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "test_size = 0.2\n",
        "num_samples = sum(1 for x in samples)\n",
        "num_training_samples = int(num_samples * (1 - test_size))\n",
        "\n",
        "train_ds = samples.take(num_training_samples)\n",
        "test_ds = samples.skip(num_training_samples)\n",
        "\n",
        "train_ds = train_ds.shuffle(BUFFER_SIZE).cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.shuffle(BUFFER_SIZE).cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "QskQkiasE1DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, test_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckB824U2F9dT",
        "outputId": "bf1cbd37-82a6-4dc6-f5e2-007560705072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 64), dtype=tf.int64, name=None), TensorSpec(shape=(None, 64), dtype=tf.int64, name=None))>,\n",
              " <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 64), dtype=tf.int64, name=None), TensorSpec(shape=(None, 64), dtype=tf.int64, name=None))>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Architecture"
      ],
      "metadata": {
        "id": "XEoNxsRkInbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers, models\n",
        "from keras import losses, optimizers, metrics"
      ],
      "metadata": {
        "id": "hK5pNHWtIDRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_DIM = 64\n",
        "VOCAB_SIZE = len(vocab)"
      ],
      "metadata": {
        "id": "npA07pvgJHAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_DIM),\n",
        "    layers.Bidirectional(layers.LSTM(32, return_sequences=True)),\n",
        "    layers.Dense(VOCAB_SIZE, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-T5VkgsI_ME",
        "outputId": "30818a1c-b5d3-431e-a240-ee0ee6d2e25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, None, 64)          2496      \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirecti  (None, None, 64)          24832     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, None, 39)          2535      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29863 (116.65 KB)\n",
            "Trainable params: 29863 (116.65 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses.SparseCategoricalCrossentropy(),\n",
        "              optimizer=optimizers.RMSprop(),\n",
        "              metrics=metrics.SparseCategoricalAccuracy())\n",
        "model.fit(train_ds,\n",
        "          epochs=10,\n",
        "          validation_data=test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvNJ28LVJoqv",
        "outputId": "4a6a16e3-3b84-463e-fb5a-7282a2e2154e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "215/215 [==============================] - 7s 15ms/step - loss: 2.7401 - sparse_categorical_accuracy: 0.2609 - val_loss: 1.8917 - val_sparse_categorical_accuracy: 0.5120\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 1.1571 - sparse_categorical_accuracy: 0.7418 - val_loss: 0.6010 - val_sparse_categorical_accuracy: 0.9036\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3458 - sparse_categorical_accuracy: 0.9506 - val_loss: 0.1802 - val_sparse_categorical_accuracy: 0.9759\n",
            "Epoch 4/10\n",
            "215/215 [==============================] - 3s 13ms/step - loss: 0.1201 - sparse_categorical_accuracy: 0.9821 - val_loss: 0.0820 - val_sparse_categorical_accuracy: 0.9861\n",
            "Epoch 5/10\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.0676 - sparse_categorical_accuracy: 0.9873 - val_loss: 0.0571 - val_sparse_categorical_accuracy: 0.9881\n",
            "Epoch 6/10\n",
            "215/215 [==============================] - 3s 13ms/step - loss: 0.0524 - sparse_categorical_accuracy: 0.9885 - val_loss: 0.0482 - val_sparse_categorical_accuracy: 0.9888\n",
            "Epoch 7/10\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.0463 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.0441 - val_sparse_categorical_accuracy: 0.9892\n",
            "Epoch 8/10\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.0433 - sparse_categorical_accuracy: 0.9890 - val_loss: 0.0419 - val_sparse_categorical_accuracy: 0.9893\n",
            "Epoch 9/10\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.0415 - sparse_categorical_accuracy: 0.9891 - val_loss: 0.0404 - val_sparse_categorical_accuracy: 0.9893\n",
            "Epoch 10/10\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.0403 - sparse_categorical_accuracy: 0.9892 - val_loss: 0.0394 - val_sparse_categorical_accuracy: 0.9894\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b1d80309a80>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predictions\n",
        "\n",
        "DRAFT - Work in progress\n",
        "\n",
        "Couldn't get the model to output properly..maybe its the shape of the predictions???"
      ],
      "metadata": {
        "id": "c0BZbQZobwbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "GsbvYEI1b6yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_chars(seed_text, next_chars, model, sequence_len):\n",
        "  for _ in range(next_chars):\n",
        "    text_to_index = np.array([[char_to_ids[char] for char in seed_text.lower()]])\n",
        "    padded_ids = pad_sequences(text_to_index, maxlen=SEQUENCE_LEN, padding=\"post\")\n",
        "    prediction = np.argmax(model.predict(text_to_index, verbose=0)[-1][-1], axis=-1)\n",
        "\n",
        "    output_text = \"\"\n",
        "    for char, idx in char_to_ids.items():\n",
        "      if idx == prediction:\n",
        "        output_text = char\n",
        "        break\n",
        "    seed_text += output_text\n",
        "  return seed_text"
      ],
      "metadata": {
        "id": "hSkLrE5iMAgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_chars(\"The course of true love never did run s\", 20, model, SEQUENCE_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "tAgb_XVaOYjI",
        "outputId": "cdd12bf0-314e-47cc-9afe-192924cb43e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The course of true love never did run sou the the the the t'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}