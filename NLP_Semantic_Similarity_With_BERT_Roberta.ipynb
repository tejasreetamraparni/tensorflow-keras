{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyM0tyiNEvxpNgqQz40iPiIP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF2UmSZeisvU",
        "outputId": "4edfb49f-4e39-4c91-bf57-5276e257910e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade keras-nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gml5tN8VHoTk",
        "outputId": "afce6658-5db3-4139-922d-0a1082de44ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.0 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "print(keras.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WKE4zCUIBco",
        "outputId": "c5cfc702-07b7-43a8-e0ae-f407497bb022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "import keras_nlp\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAe8C79si6sp",
        "outputId": "422e1046-fc87-420b-be97-1ef9f49ed372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Dataset used is SNLI - Stanford Natural Language Inference.\n",
        "Load using Tensorflow datasets.\n",
        "Contains over 550k samples in total. We'll be using only 20% of training data\n",
        "\n",
        "A little about the dataset:\n",
        "contains 3 part to each sample: hypothesis, premise and label\n",
        "* Hypothesis: Hypothesis created by the author\n",
        "* Premise: original caption provided to the author\n",
        "* Label: Annoted to indicate the similary b/w two sentences which can be one of the three:\n",
        "  - Contradiction (completely dissimilar sentences)\n",
        "  - Entailment (similar sentences)\n",
        "  - Neutral (clear similarity or dissimilarity cannot be established)"
      ],
      "metadata": {
        "id": "eU_ugu0ujsZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: changed to 1000 samples as model wasn't training\n",
        "snli_train = tfds.load(\"snli\", split=\"train[:1000]\")\n",
        "snli_val = tfds.load(\"snli\", split=\"validation[:500]\")\n",
        "snli_test = tfds.load(\"snli\", split=\"test[:500]\")\n",
        "\n",
        "# above load the data into tf.data.Dataset objects"
      ],
      "metadata": {
        "id": "ngRw8Ip_kFqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = snli_train.batch(4).take(1).get_single_element()\n",
        "sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCZb8CJOpjpP",
        "outputId": "e864bca6-769d-49d5-9022-ba8c868e153b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hypothesis': <tf.Tensor: shape=(4,), dtype=string, numpy=\n",
              " array([b'Washing clothes on a camping trip.', b'A woman walking alone.',\n",
              "        b'The woman is thinking.',\n",
              "        b'The woman is practicing to enter a roller derby.'], dtype=object)>,\n",
              " 'label': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 1,  2, -1,  1])>,\n",
              " 'premise': <tf.Tensor: shape=(4,), dtype=string, numpy=\n",
              " array([b'A man washes or dies clothes in a primitive setting.',\n",
              "        b'A woman is walking her baby with a stroller at the local park.',\n",
              "        b'This is a pensive women in a grassy setting.',\n",
              "        b'Woman in red shirt and white cap rollerblading on gray surface.'],\n",
              "       dtype=object)>}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "rboDbl-6qP0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# there are labels with missing or incorrect labels: label=-1. Remove these\n",
        "def filter_labels(sample):\n",
        "  return sample[\"label\"] >= 0\n",
        "\n",
        "#utility function that splits the example into (x, y) tuple\n",
        "def split_labels(sample):\n",
        "  x = (sample[\"hypothesis\"], sample[\"premise\"])\n",
        "  y = sample[\"label\"]\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "c-rFgTnZuqO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = (\n",
        "    snli_train.filter(\n",
        "        filter_labels)\n",
        "    .map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(16)\n",
        ")\n",
        "\n",
        "val_ds = (\n",
        "    snli_val\n",
        "    .filter(filter_labels)\n",
        "    .map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(16)\n",
        ")\n",
        "\n",
        "test_ds = (\n",
        "    snli_test\n",
        "    .filter(filter_labels)\n",
        "    .map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(16)\n",
        ")"
      ],
      "metadata": {
        "id": "kaOK4QC1vbuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Establish baseline with BERT"
      ],
      "metadata": {
        "id": "X7oKpuOuz_AN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    num_classes=3\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4secOZs0oNX",
        "outputId": "2b6b19e1-b8a1-419c-9e28-5acf2452b2cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased/2/download/config.json...\n",
            "100%|██████████| 507/507 [00:00<00:00, 331kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased/2/download/model.weights.h5...\n",
            "100%|██████████| 16.8M/16.8M [00:01<00:00, 15.2MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py:46: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py:46: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased/2/download/tokenizer.json...\n",
            "100%|██████████| 547/547 [00:00<00:00, 532kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased/2/download/assets/tokenizer/vocabulary.txt...\n",
            "100%|██████████| 226k/226k [00:00<00:00, 697kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_classifier.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "8nWl2gxK1Pna",
        "outputId": "83751064-8d9d-4b2f-85fe-5af0de09c5b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"bert_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"bert_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ bert_tokenizer (\u001b[38;5;33mBertTokenizer\u001b[0m)                     │                                              \u001b[38;5;34m30,522\u001b[0m │\n",
              "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ bert_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertTokenizer</span>)                     │                                              <span style=\"color: #00af00; text-decoration-color: #00af00\">30,522</span> │\n",
              "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"bert_classifier\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"bert_classifier\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ segment_ids (\u001b[38;5;33mInputLayer\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ bert_backbone (\u001b[38;5;33mBertBackbone\u001b[0m)                  │ {sequence_output: (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),   │       \u001b[38;5;34m4,385,920\u001b[0m │\n",
              "│                                               │ pooled_output: (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)}            │                 │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ classifier_dropout (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ logits (\u001b[38;5;33mDense\u001b[0m)                                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                              │             \u001b[38;5;34m387\u001b[0m │\n",
              "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                  </span>┃<span style=\"font-weight: bold\"> Output Shape                           </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ segment_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ bert_backbone (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertBackbone</span>)                  │ {sequence_output: (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,385,920</span> │\n",
              "│                                               │ pooled_output: (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)}            │                 │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ classifier_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ logits (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
              "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,386,307\u001b[0m (16.73 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,386,307</span> (16.73 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,386,307\u001b[0m (16.73 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,386,307</span> (16.73 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_classifier.fit(train_ds,\n",
        "                   validation_data=val_ds,\n",
        "                   epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyAf--1c1i2B",
        "outputId": "5c622fd2-79d3-4100-b753-d0809d232d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 25s 394ms/step - loss: 1.0763 - sparse_categorical_accuracy: 0.4509 - val_loss: 1.0747 - val_sparse_categorical_accuracy: 0.4433\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79e7bd9ccc10>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate performance on test data"
      ],
      "metadata": {
        "id": "bJZM9XBU8SgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_classifier.evaluate(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWUxS03U8mNP",
        "outputId": "21143c5b-51fa-4b72-e1e6-542318d1e0af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31/31 [==============================] - 3s 33ms/step - loss: 1.0704 - sparse_categorical_accuracy: 0.4726\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0703612565994263, 0.47261664271354675]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recompile the model with slightly different learning rate"
      ],
      "metadata": {
        "id": "B40a1uER8qmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    num_classes=3\n",
        ")\n",
        "\n",
        "bert_classifier.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "bert_classifier.fit(train_ds, validation_data=val_ds, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh431l6G828H",
        "outputId": "a8bb75f7-2008-4974-adb6-16502f30df6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py:46: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py:46: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 19s 139ms/step - loss: 1.0912 - accuracy: 0.4018 - val_loss: 1.0755 - val_accuracy: 0.4577\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79e7bd240790>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_classifier.evaluate(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px0H8hMr9aQy",
        "outputId": "2edc4e9a-7ee2-4754-cf3e-004ae8e6c418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31/31 [==============================] - 1s 13ms/step - loss: 1.0716 - accuracy: 0.4807\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0716173648834229, 0.48073023557662964]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recompile with new optimizer AdamW and learning rate schedule\n",
        "\n",
        "We're using Triangular scheduling which follows the below pattern:\n",
        "\n",
        "* Increase learning rate gradually upto a certain number of epochs to a maximum value\n",
        "* Then, drop at the same rate and epochs to a minimum value\n",
        "\n",
        "This algorithm helps to explore different learning rate across different regions and generalizes better.\n",
        "\n",
        "In formal terms, linear ramp up for \"warmup\" steps, then linear decay to zero at \"total\" steps"
      ],
      "metadata": {
        "id": "gpbvBz3U9pxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TriangularSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, rate, warmup, total):\n",
        "    self.rate = rate\n",
        "    self.warmup = warmup\n",
        "    self.total = total\n",
        "\n",
        "  def get_config(self):\n",
        "    config = {\"rate\":self.rate, \"warmup\":self.warmup, \"total\":self.total}\n",
        "    return config\n",
        "\n",
        "  def __call__(self, step): # what happens when the function is called first time\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    rate = tf.cast(self.rate, dtype=tf.float32)\n",
        "    warmup = tf.cast(self.warmup, dtype=tf.float32)\n",
        "    total = tf.cast(self.total, dtype=tf.float32)\n",
        "\n",
        "    warmup_rate = rate * step / warmup\n",
        "    cooldown_rate = rate * (total - step) / (total - warmup)\n",
        "    triangular_rate = tf.math.minimum(warmup_rate, cooldown_rate)\n",
        "    return tf.math.maximum(triangular_rate, 0.0)\n"
      ],
      "metadata": {
        "id": "8-LrdaCa92Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example of how rate progresses\n",
        "total_steps = 20\n",
        "warm_steps = 10\n",
        "my_rate = 2\n",
        "my_step = 1\n",
        "warm = []\n",
        "cool = []\n",
        "triangle = []\n",
        "for i in range(1, warm_steps+1):\n",
        "  warm_rate = my_rate * i / warm_steps\n",
        "  warm.append(warm_rate)\n",
        "for i in range(warm_steps+1, total_steps+1):\n",
        "  cool_rate = my_rate * (total_steps - i) / (total_steps - warm_steps)\n",
        "  cool.append(cool_rate)"
      ],
      "metadata": {
        "id": "VBlPEOm8BJxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(warm)\n",
        "print(cool)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a-qUcjhCuhw",
        "outputId": "f27bce51-461e-4028-a156-3233f353227f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]\n",
            "[1.8, 1.6, 1.4, 1.2, 1.0, 0.8, 0.6, 0.4, 0.2, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    num_classes=3\n",
        ")\n",
        "\n",
        "epochs = 3\n",
        "total_steps = sum(1 for _ in train_ds.as_numpy_iterator()) * epochs\n",
        "warmup_steps = int(total_steps * 0.2)\n",
        "\n",
        "bert_classifier.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.AdamW(\n",
        "        TriangularSchedule(1e-4, warmup_steps, total_steps)\n",
        "    ),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "bert_classifier.fit(train_ds,\n",
        "                    validation_data=val_ds,\n",
        "                    epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nc1kP4G4ERfA",
        "outputId": "0e959765-b076-4c77-9139-61801bef114a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py:46: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py:46: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "63/63 [==============================] - 15s 128ms/step - loss: 1.0902 - accuracy: 0.3828 - val_loss: 1.0791 - val_accuracy: 0.4454\n",
            "Epoch 2/3\n",
            "63/63 [==============================] - 12s 188ms/step - loss: 1.0635 - accuracy: 0.4579 - val_loss: 1.0643 - val_accuracy: 0.4784\n",
            "Epoch 3/3\n",
            "63/63 [==============================] - 4s 68ms/step - loss: 1.0323 - accuracy: 0.4920 - val_loss: 1.0615 - val_accuracy: 0.4536\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79e7bdbf9540>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.math.maximum(2, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "za0ANtcXEtqC",
        "outputId": "3b8ec71e-c040-4f7f-80de-a86b7e47c84d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=4>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate\n",
        "bert_classifier.evaluate(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN-TBGBhGIt8",
        "outputId": "179b9234-4232-437e-e9b9-01996ea02c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31/31 [==============================] - 1s 9ms/step - loss: 1.0454 - accuracy: 0.4625\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0454307794570923, 0.46247464418411255]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save and reload the model"
      ],
      "metadata": {
        "id": "rwRMnk0eLU2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_classifier.save(\"bert_classifier.keras\")\n",
        "restored_model = keras.models.load_model(\"bert_classifier.keras\")\n",
        "restored_model.evaluate(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7BE5OSNLexP",
        "outputId": "75934786-cbc5-412b-96e6-f8736d64058b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/task.py:54: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/task.py:54: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py:46: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py:46: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "WARNING:tensorflow:`compile()` was not called as part of model loading because the model's `compile()` method is custom. All subclassed Models that have `compile()` overridden should also override `get_compile_config()` and `compile_from_config(config)`. Alternatively, you can call `compile()` manually after loading.\n",
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 1 variables whereas the saved optimizer has 83 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31/31 [==============================] - 3s 25ms/step - loss: 1.0454 - sparse_categorical_accuracy: 0.4625\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0454306602478027, 0.46247464418411255]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performing Inference with the model"
      ],
      "metadata": {
        "id": "4p9K2-RqLomC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = (sample[\"hypothesis\"], sample[\"premise\"])\n",
        "sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSojnwfhLxtK",
        "outputId": "2bc010e7-608c-406f-c376-725c997e294a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(4,), dtype=string, numpy=\n",
              " array([b'Washing clothes on a camping trip.', b'A woman walking alone.',\n",
              "        b'The woman is thinking.',\n",
              "        b'The woman is practicing to enter a roller derby.'], dtype=object)>,\n",
              " <tf.Tensor: shape=(4,), dtype=string, numpy=\n",
              " array([b'A man washes or dies clothes in a primitive setting.',\n",
              "        b'A woman is walking her baby with a stroller at the local park.',\n",
              "        b'This is a pensive women in a grassy setting.',\n",
              "        b'Woman in red shirt and white cap rollerblading on gray surface.'],\n",
              "       dtype=object)>)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = bert_classifier.predict(sample)\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x) / np.exp(x).sum(axis=0)\n",
        "\n",
        "predictions = softmax(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWh9ESQYL2sx",
        "outputId": "22dba7ea-a895-4c49-bd5d-16c5f346129a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x79e7a7adbeb0> and will run it as-is.\n",
            "Cause: Unable to locate the source code of <function Model.make_predict_function.<locals>.predict_function at 0x79e7a7adbeb0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x79e7a7adbeb0> and will run it as-is.\n",
            "Cause: Unable to locate the source code of <function Model.make_predict_function.<locals>.predict_function at 0x79e7a7adbeb0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1/1 [==============================] - 1s 1s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(predictions, axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dJsvhgTMGhj",
        "outputId": "e7e39b68-f520-44d1-862d-f16356322c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 2, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax([[2, 4], [0, 10]], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN-CXrrFMVkF",
        "outputId": "c1547a86-74ce-4f1d-88e6-253013d2bb04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improvign with RoBERTa"
      ],
      "metadata": {
        "id": "Lo1dMJJOMgcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_classifier = keras_nlp.models.RobertaClassifier.from_preset(\n",
        "    \"roberta_base_en\",\n",
        "    num_classes=3\n",
        ")\n",
        "\n",
        "roberta_classifier.fit(train_ds,\n",
        "                       validation_data=val_ds,\n",
        "                       epochs=2)\n",
        "roberta_classifier.evaluate(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFHW-zOlM9tX",
        "outputId": "29050cf1-ac37-4ff0-b615-aab681f44066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/roberta/keras/roberta_base_en/2/download/config.json...\n",
            "100%|██████████| 498/498 [00:00<00:00, 1.16MB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/roberta/keras/roberta_base_en/2/download/model.weights.h5...\n",
            "100%|██████████| 474M/474M [00:13<00:00, 36.4MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py:46: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py:46: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/roberta/keras/roberta_base_en/2/download/tokenizer.json...\n",
            "100%|██████████| 463/463 [00:00<00:00, 494kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/roberta/keras/roberta_base_en/2/download/assets/tokenizer/merges.txt...\n",
            "100%|██████████| 446k/446k [00:00<00:00, 988kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/roberta/keras/roberta_base_en/2/download/assets/tokenizer/vocabulary.json...\n",
            "100%|██████████| 0.99M/0.99M [00:00<00:00, 1.78MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "63/63 [==============================] - 133s 1s/step - loss: 1.1015 - sparse_categorical_accuracy: 0.3417 - val_loss: 1.1000 - val_sparse_categorical_accuracy: 0.3361\n",
            "Epoch 2/2\n",
            "63/63 [==============================] - 50s 795ms/step - loss: 1.0598 - sparse_categorical_accuracy: 0.4248 - val_loss: 0.9207 - val_sparse_categorical_accuracy: 0.5691\n",
            "31/31 [==============================] - 14s 243ms/step - loss: 0.9149 - sparse_categorical_accuracy: 0.5882\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9148991703987122, 0.5882353186607361]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = roberta_classifier.predict(sample)\n",
        "print(np.argmax(predictions, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfElWCl0NOyK",
        "outputId": "44e7e7ad-13e9-424f-a9de-c0baad49b0b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x79e7aa0fb7f0> and will run it as-is.\n",
            "Cause: Unable to locate the source code of <function Model.make_predict_function.<locals>.predict_function at 0x79e7aa0fb7f0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x79e7aa0fb7f0> and will run it as-is.\n",
            "Cause: Unable to locate the source code of <function Model.make_predict_function.<locals>.predict_function at 0x79e7aa0fb7f0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.step_function.<locals>.run_step at 0x79e7aa3a2e60> and will run it as-is.\n",
            "Cause: Unable to locate the source code of <function Model.make_predict_function.<locals>.step_function.<locals>.run_step at 0x79e7aa3a2e60>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.step_function.<locals>.run_step at 0x79e7aa3a2e60> and will run it as-is.\n",
            "Cause: Unable to locate the source code of <function Model.make_predict_function.<locals>.step_function.<locals>.run_step at 0x79e7aa3a2e60>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1/1 [==============================] - 12s 12s/step\n",
            "[0 0 0 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BF0X7RO7OuEq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}